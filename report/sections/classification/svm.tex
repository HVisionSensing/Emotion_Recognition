\phantomsection
\chapter{Support Vector Machines}
\label{chap:svm}

\noindent Support Vector Machines (SVM) is a binary linear classifier belonging to the supervised learning algorithms category. It has been proven to be very efficient for classification, regression and novelty detection problems \cite{BIS06}, which makes it suitable for facial expression recognition.
\newline

\noindent This chapter will first provide an overview of how classification is performed using SVM. It will then focus and describe the key points behind this classifier, namely \textit{margin maximization} and \textit{kernel function}. The chapter will conclude with the review of a research paper detailing facial expression recognition using LBP for feature extraction and SVM for classification.
\newline

\phantomsection
\section{Overview}
\label{svm_overview}

\vspace{\baselineskip}
\noindent SVM is originally a binary classifier, which means it has a \textit{one-vs-all} approach. It is a decision machine, so its output is not a posterior probability, but rather a class label \cite{BIS06}.  It can be adapted into a \textit{relevance vector machine} to output posterior probabilities though \cite{BIS06}, but this alternative will not be detailed in our report.
\newline

\noindent SVM is also a linear classifier, such as LDA. For a two-class problem, it will linearly separate the train data by finding the optimal hyperplane between these 2 classes. This hyperplane is defined as being as far as possible from both classes. \textit{Margin maximization} is then applied to optimize the distance between the two classes and the hyperplane, as it will be explained in Section \ref{margin_max}.
\newline

\noindent The name "Support Vector" originates from this margin maximization feature. Indeed, the distance between the classes and the separating hyperplane is computed regarding to the closest data points from both classes. These particular data points, lying on the margins edges, are the support vectors. Some properties are associated to these vectors, such as non-zero Lagrange multipliers (see Section \ref{margin_max}), which builds the classification algorithm.
\newline

\noindent For a multi-class problem, however, this linear separation is not possible anymore. The dataset has to be mapped into an other space, where it can be linearly separated. This is what \textit{kernel functions} are used for, as described in Section \ref{kernel_fct}.
\newline

\phantomsection
\section{Margin maximization}
\label{margin_max}

\vspace{\baselineskip}
\noindent As introduced in Section \ref{svm_overview}, margin maximization for a two-class linear problem starts by finding the separating hyperplane between these two classes. A linear classification model of the form $f(x) = w(x) + b$ can be inferred, with $w$ being the normal to the hyperplane and $b$ the bias. The hyperplane can then be characterized by  $w(x) + b = 0$.
\newline

\noindent Margin is defined as the distance between the closest point of the class to the hyperplane, and that hyperplane, which can also be written as $ d(x) = \frac{|w(x) + b |}{||w||}$. Since a data point $(x_i, y_i)$ is correctly classified if $y_if(x_i) \geq 1 (1)$, maximizing the margin is the action of maximizing $||w||^{-1}$, which is consequently equivalent to minimizing $||w||^2$ depending on constraint $(1)$. Margin maximization then requires to solve a \textit{quadratic programming} problem under constraints, as seen in Equation \ref{margin_max_eq}.

\begin{equation}
\left\{
\begin{array}{l}
\min \frac{1}{2} ||w||^2 \\
\forall i, \, y_i . f(x) \geq 1
\end{array}
\right.
\label{margin_max_eq}
\end{equation}
\vspace{\baselineskip}

\noindent In order to solve this problem, positive \textit{Lagrangian coefficients} $\alpha_i$, $i=1..N$ are introduced for each constraint present in Equation \ref{margin_max_eq}. It outputs a Lagrangian function of $w$ and $b$:

\begin{equation}
L_P(w, b,\alpha) = \frac{1}{2}||w||^2 - \sum\limits_{i=1}\limits^{N}\alpha_i\left(y_i(w.x_i + b) -1\right)
\label{lagrangian}
\end{equation}
\vspace{\baselineskip}

\noindent With $\alpha = (\alpha_1..\alpha_N)^T$. When setting $\frac{\delta L}{\delta w} = 0$ and $\frac{\delta L}{\delta b} = 0$ in order to minimize $L_p$ with respect to $w$ and $b$, two conditions can be obtained
\newline

\begin{equation}
w = \sum\limits_{i=1}\limits{N} \alpha_i y_i x_i
\end{equation}

and

\begin{equation}
\sum\limits_{i=1}\limits{N} \alpha_i y_i = 0.
\end{equation}
\vspace{\baselineskip}

\noindent When substituting these conditions into Equation \ref{lagrangian}, parameters $w$ and $b$ disappear. Thus the resulting function is Equation \ref{lagrangian2}
\newline

\begin{equation}
L_D = \sum\limits_{i=1}\limits{N} \alpha_i - \frac{1}{2} \sum\limits_{i=1}\limits{N} \sum\limits_{j=1}\limits{N} \alpha_i \alpha_j y_i y_j(x_i.x_j)
\label{lagrangian2}
\end{equation}
\vspace{\baselineskip}

\noindent This function is called \textit{dual representation} of the margin maximization problem. In order to solve this problem, one can either minimize $L_P$ in Equation \ref{lagrangian} or maximize $L_D$ in Equation \ref{lagrangian2}.
\newline

\phantomsection
\section{Kernel function}
\label{kernel_fct}

\vspace{\baselineskip}
\noindent It might however not be possible to perform linear separation with more classes. Indeed, data might be overlapping, and thus it will not be a linear problem anymore. The solution to overcome this problem is to map the non-linear dataset from its input space into a higher feature space using a function $\Phi(x)$, and perform margin maximization and classification in this higher space. 
\newline

\noindent In order to achieve this mapping, a \textit{kernel function} of the form $K(x_i, x_j) = \Phi(x_i)^T \Phi(x_j)$ is applied to the dataset. This kernel represents an inner product in the feature space. There are four kernels available, which are described in Equation \ref{kernels_svm}.
\newline

\begin{equation}
\begin{array}{ll}
	\text{Linear kernel:} & K(x_i,x_j) = x_i^Tx \\
	\text{Polynomial kernel:} & K(x_i,x_j) = (\gamma x_i^Tx_j + r)^T, \gamma > 0 \\
	\text{Radial Basis Function (Gaussian) kernel:} & K(x_i,x_j) = \exp(-\gamma \| x_i - x_j \|^2), \gamma > 0 \\
	\text{Sigmoid kernel:} & K(x_i,x_j) = \tanh(\gamma x_i^T x_j + r)\\
\end{array}
\label{kernels_svm}
\end{equation}

\vspace{\baselineskip}

\noindent The main advantage of using a kernel function is that there is no need to define or calculate $\Phi(x_i)$, only $\Phi(x_i)^T \Phi(x_j)$. We hence do not know the true form of $\Phi(x_i)$. Despite having an incomplete knowledge of $K(x_i,x_j)$, the four kernels described above can be used as they are, or be combined in order to build more complex kernels.
\newline

\phantomsection
\section{Combining LBP and SVM}

\vspace{\baselineskip}
<<<<<<< HEAD
\noindent In a 2009 article, Shang and al \cite{SHA09} have performed facial expression recognition while using Local Binary Patterns for feature extraction, and compared the accuracy of different kinds of classifiers:  template matching,  LDA and SVM. The template-matching classifier is a nearest-neighbour classifier based on \textit{Chi square statistic} ($\chi^2$) dissimilarity measure. Secondly, train data for LDA was first mapped into a PCA subspace. Even though the dimensionality of the data was reduced, 98\% of the information was kept \cite{SHA09}. It was then combined with a Neural Network (NN) classifier. About SVM, since it is a binary classifier, multi-class classification was achieved through a \textit{one-versus-the-rest} approach. It is the same as for K-means, as presented in Chapter \ref{chap:classification}, Section \ref{unsup_learning}. They also used 10-fold cross-validation, and performed grid search to find optimized parameters. Images from the Cohn-Kanade database were used as train and test data.
\newline

\noindent The output of their study is that classification using SVM has a higher accuracy rate than the two other methods. Template matching yields 84.5\% of accuracy when performing 6-class recognition, and 79.1\% for 7-class recognition. SVM outperforms largely this method by having an accuracy rate between 91.6\% and 92.5\% for 6-class recognition depending on the kernel used, and between 88.1\% and 88.9\% for 7-class recognition \cite{SHA09}. When comparing LDA and SVM, since LDA was trained on data mapped into another subspace, SVM classification was also performed on the same subspace. The combination of LDA and NN has accuracy rates of 79.2\% and 73.4\%, respectively for 6-class and 7-class recognition. It can already be inferred that LDA+NN perform worse than template matching when provided with appropriate parameters and data. Then, with SVM trained on the same subspace and with the linear kernel, the accuracy rate is 87.7\% (6-class) and 80.2\% (7-class) \cite{SHA09}.
\newline

\noindent They also used a linear programming classification method which was presented in 2005 \cite{FEN05}, but it will not be detailed in this report. The general outcome is that SVM outperforms other classification methods, which makes it very efficient for facial expression recognition.
\newline

\noindent This study also compares the efficiency of Local Binary Patterns and Gabor wavelets, when combined with SVM. Gabor wavelets yield accuracy rates generally 2\% lower than LBP-based feature extraction, no matter which SVM kernel is used. Indeed, LBP feature extraction has an accuracy rate between 91.5\% and 92.6\% for 6-class recognition, while Gabor wavelets feature extraction results in accuracy rates ranging from 89.4\% to 89.8\% \cite{SHA09}. For 7-class recognition, Gabor wavelets accuracy ranges between 86.6\% and 86.8\%, while LBP-based feature extraction has an accuracy rate between 88.1\% and 88.9\% \cite{SHA09}. The conclusion is that the combination of LBP and SVM, when provided with suitable parameters, has a very high accuracy rate for facial expression recognition.
\newline

\noindent However, the accuracy for each facial expression varies a lot. When used with RBF kernel, SVM has some difficulties especially when it comes to distinguish fear and sadness, the two facial expressions which have the lowest accuracy rates (6-class recognition: 73.0\% and 83.5\% respectively; 7-class recognition: 68.0\% and 69.5\% respectively). Indeed, expression recognition accuracy range from 88.9\% (anger) to 98.7\% (surprise) in 6-class recognition, and from 85.0\% (anger) to 98.2\% (surprise) for 7-class recognition.
\newline

\noindent Since the accuracy of the system presented in this article is very high, we chose to implement a similar system. Indeed, we are performing facial expression recognition using LBP for feature extraction, and SVM classification. We however did not use the Cohn-Kanade database as train data because we did not get it on time. We will describe our implementation and results further in the report. 
=======
\noindent In a 2009 article, Shang and al \cite{SHA09} have performed facial expression recognition while using Local Binary Patterns for feature extraction, and compared the accuracy of different kinds of classifiers:  template matching,  LDA and SVM. They have used images from the Cohn-Kanade database as train data, and the conclusion of their study is that classification using SVM has a higher accuracy rate than the two other methods. Template matching 
\newline

\noindent Gabor wavelets \& lbp
\newline

\noindent The accuracy for each facial expression is however not the same. SVM has some difficulties especially when it comes to distinguish fear and sadness, the two facial expressions which have the lowest accuracy rates. Fear is mistaken with joy, while sadness is mistaken with anger or neutral state. Recognitions rates are however usually better for a 7-class classification, except for fear.
\newline

\noindent BLA BLA BLA
\newline

\noindent Since the accuracy of the system presented in this article is very high, we chose to implement a similar system. Indeed, we are performing facial expression recognition using LBP for feature extraction, and SVM classification. We however did not use the Cohn-Kanade database as train data. We will describe further our implementation and results further in the report. 
>>>>>>> f3a1177041b834dbad80cc320be587f28fef16e6
\newline

