\phantomsection
\chapter{Support Vector Machine}
\label{chap:svm}

\noindent Support Vector Machine (SVM) is a binary linear classifier belonging to the supervised learning algorithms category. It has been proven to be very efficient for classification, regression and novelty detection problems \cite{BIS06}, which makes it suitable for facial expression recognition.
\newline

\noindent This chapter will first provide an overview of how classification is performed using SVM. It will then focus and describe the key points behind this classifier, namely \textit{margin maximization} and \textit{kernel function}. The chapter will conclude with the review of a research paper detailing facial expression recognition using LBP for feature extraction and SVM for classification.
\newline

\phantomsection
\section{Overview}
\label{svm_overview}

\vspace{\baselineskip}
\noindent SVM is originally a binary classifier, which means it has a \textit{one-vs-all} approach. It is a decision machine, so its output is not a posterior probability, but rather a class label \cite{BIS06}.  It can be adapted into a \textit{relevance vector machine} to output posterior probabilities though \cite{BIS06}, but this alternative will not be detailed in our report.
\newline

\noindent SVM is also a linear classifier, such as LDA. For a two-class problem, it will linearly separate the train data by finding the optimal hyperplane between these 2 classes. This hyperplane is defined as being as far as possible from both classes. \textit{Margin maximization} is then applied to optimize the distance between the two classes and the hyperplane, as it will be explained in Section \ref{margin_max}.
\newline

\noindent The name "Support Vector" originates from this margin maximization feature. Indeed, the distance between the classes and the separating hyperplane is computed regarding to the closest data points from both classes. These particular data points, lying on the margins edges, are the support vectors. %Some properties are associated to these vectors, such as non-zero Lagrange multipliers (see Section \ref{margin_max}), which builds the classification algorithm.
\newline

\noindent For a multi-class problem, however, this linear separation is not possible anymore. The dataset has to be mapped into an other space, where it can be linearly separated. This is what \textit{kernel functions} are used for, as described in Section \ref{kernel_fct}.
\newline

\phantomsection
\section{Margin maximization}
\label{margin_max}

\vspace{\baselineskip}
\noindent As introduced in Section \ref{svm_overview}, margin maximization for a two-class linear problem starts by finding the separating hyperplane between these two classes. A linear classification model of the form $f(x) = w(x) + b$ can be inferred, with $w$ being the normal to the hyperplane and $b$ the bias. The hyperplane can then be characterized by  $w(x) + b = 0$.
\newline

\noindent Margin is defined as the distance between the closest point of the class to the hyperplane, and that hyperplane, which can also be written as $ d(x) = \frac{|w(x) + b |}{||w||}$. Since a data point $(x_i, y_i)$ is correctly classified if $y_if(x_i) \geq 1 (1)$, maximizing the margin is the action of maximizing $||w||^{-1}$, which is consequently equivalent to minimizing $||w||^2$ depending on constraint $(1)$. Margin maximization then requires to solve a \textit{quadratic programming} problem under constraints, as seen in Equation \ref{margin_max_eq}.

\begin{equation}
\left\{
\begin{array}{l}
\min \frac{1}{2} ||w||^2 \\
\forall i, \, y_i . f(x) \geq 1
\end{array}
\right.
\label{margin_max_eq}
\end{equation}

%\noindent lagrange multipliers
%\newline

\vspace{\baselineskip}

\phantomsection
\section{Kernel function}
\label{kernel_fct}

\vspace{\baselineskip}
\noindent It might however not be possible to perform linear separation with more classes. Indeed, data might be overlapping, and thus it will not be a linear problem anymore. The solution to overcome this problem is to map the non-linear dataset from its input space into a higher feature space using a function $\Phi(x)$, and perform margin maximization and classification in this higher space. 
\newline

\noindent In order to achieve this mapping, a \textit{kernel function} of the form $K(x_i, x_j) = \Phi(x_i)^T \Phi(x_j)$ is applied to the dataset. This kernel represents an inner product in the feature space. There are four kernels available, which are described in Equation \ref{kernels_svm}.
\newline

\begin{equation}
\begin{array}{ll}
	\text{Linear kernel:} & K(x_i,x_j) = x_i^Tx \\
	\text{Polynomial kernel:} & K(x_i,x_j) = (\gamma x_i^Tx_j + r)^T, \gamma > 0 \\
	\text{Radial Basis Function (Gaussian) kernel:} & K(x_i,x_j) = \exp(-\gamma \| x_i - x_j \|^2), \gamma > 0 \\
	\text{Sigmoid kernel:} & K(x_i,x_j) = \tanh(\gamma x_i^T x_j + r)\\
\end{array}
\label{kernels_svm}
\end{equation}

\vspace{\baselineskip}

\noindent The main advantage of using a kernel function is that there is no need to define or calculate $\Phi(x_i)$, only $\Phi(x_i)^T \Phi(x_j)$. We hence do not know the true form of $\Phi(x_i)$. However, simple kernels are usually combined in order to build more complex ones.
\newline

\phantomsection
\section{Combining LBP and SVM}

\vspace{\baselineskip}
\noindent In a 2009 article, Shang and al \cite{SHA09} have performed facial expression recognition while using Local Binary Patterns for feature extraction, and compared the accuracy of different kinds of classifiers:  template matching,  LDA and SVM. They have used images from the Cohn-Kanade database as train data, and the conclusion of their study is that classification using SVM has a high accuracy rate, as seen in Table \ref{accuracy_svm_lbp}. 
\newline

\begin{table}[h]
\begin{tabular}{|lcc|}
\hline
 & 6-Class recognition (\%) &  7-Class recognition (\%) \\
 \hline
 SVM (linear) & 91.5 $\pm$ 3.1 & 88.1 $\pm$ 3.8 \\
 SVM (polynomial) & 91.5 $\pm$ 3.1 & 88.1 $\pm$ 3.8 \\
 SVM (RBF) & 92.6 $\pm$ 3.1 & 88.9 $\pm$ 3.5 \\
 \hline
\end{tabular}
\caption{\label{accuracy_svm_lbp} Recognition performance of LBP-based SVM with different kernels}
\end{table}

\noindent However, as seen in confusion matrices \ref{conf_mtx_6_svm_lbp} and \ref{conf_mtx_7_svm_lbp}, the accuracy for each facial expression is not the same. SVM has some difficulties especially when it comes to distinguish fear and sadness, the two facial expressions which have the lowest accuracy rates. Fear is mistaken with joy, while sadness is mistaken with anger or neutral state. Recognitions rates are however usually better for a 7-class classification, except for fear.
\newline

\begin{table}[h]
\begin{tabular}{|lcccccc|}
\hline
 & Anger & Disgust & Fear & Joy & Sadness & Surprise \\
\hline
Anger & 89.7\% & 2.7\% & 0\% & 0\% & 7.6\% & 0\% \\
Disgust & 0\% & 97.5\% & 2.5\% & 0\% & 0\% & 0\% \\
Fear & 0\% & 2.0\% & 73.0\% & 22.0\% & 3.0\% & 0\% \\
Joy & 0\% & 0.4\% & 0.7\% & 97.9\% & 1.0\% & 0\% \\
Sadness & 10.3\% & 0\% & 0.8\% & 0.8\% & 83.5\% & 4.6\% \\
Surprise & 0\% & 0\% & 1.3\% & 0\% & 0\% & 98.7\% \\
\hline
\end{tabular}
\caption{\label{conf_mtx_6_svm_lbp} Confusion matrix of 6-class facial expression recognition using SVM (RBF)}
\end{table}

\noindent Since the accuracy of the system presented in this article is very high, we chose to implement a similar system. Indeed, we are performing facial expression recognition using LBP for feature extraction, and SVM classification. We however did not use the Cohn-Kanade database as train data. We will describe further our implementation and results further in the report. 
\newline

\begin{table}[h]
\begin{tabular}{|lccccccc|}
\hline
& Anger & Disgust & Fear & Joy & Sadness & Surprise & Neutral \\
\hline
Anger & 85.0\% & 2.7\% & 0\% & 0\% & 4.8\% & 0\% & 7.5\% \\
Disgust & 0\% & 97.5\% & 2.5\% & 0\% & 0\% & 0\% & 0\% \\
Fear & 0\% & 2.0\% & 68.0\% & 22.0\% & 1.0\% & 0\% & 7.0\% \\
Joy & 0\% & 0\% & 0.7\% & 94.7 \% & 1.1\% & 0\% & 3.5\% \\
Sadness & 8.6\% & 0\% & 0\% & 0\% & 69.5\% & 2.3\% & 19.6\% \\
Surprise & 0\% & 0\% & 1.3\% & 0\% & 0\% & 98.2\% & 0.5\% \\
Neutral & 1.6\% & 0.4\% & 0\% & 1.6\% & 6.0\% & 0.4\% & 90.0\% \\
\hline
\end{tabular}
\caption{\label{conf_mtx_7_svm_lbp} Confusion matrix of 7-class facial expression recognition using SVM (RBF)}
\end{table}
\clearpage
