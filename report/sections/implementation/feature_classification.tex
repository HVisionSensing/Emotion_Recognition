\phantomsection
\chapter{Classification with Support Vector Machines}
\label{chap:implementation_svm}

\noindent The classification part for this facial expression recognition system uses a Support Vector Machines (SVM) classifier. This chapter describes how SVM is used and implemented
\newline

\phantomsection
\section{Histogram concatenation}

\vspace{\baselineskip}
\noindent The output of the feature extraction part using Local Binary Patterns (LBP) is a feature vector characterizing the whole image, made of histograms characterizing each region of the face image. In order to proceed to the classification part, the feature vector has to be concatenated to be readable by SVM functions.
\newline

\noindent For the training part, the feature vector has to be in the following form:
\begin{center}
\noindent $ l $ \hspace{0.7cm} $ i:v $ \hspace{0.2cm} $ i:v $ \hspace{0.2cm} $ ... $ \hspace{0.2cm} $ -1:-1 $
\end{center} 
\noindent $ l $ stands for the label of the class the image belongs to. A label is a number associated to the 6 emotions, plus neutral state. These emotions have been ordered alphabetically and bear a number between 1 and 6, the neutral state having value 0. Values corresponding to the emotions are then assigned as following:

\begin{itemize}
	\item \textit{Neutral} state: $ 0 $
	\item \textit{Afraid} state: $ 1 $
	\item \textit{Angry} state: $ 2 $
	\item \textit{Disgusted} state: $ 3 $
	\item \textit{Happy} state: $ 4 $
	\item \textit{Sad} state: $ 5 $
	\item \textit{Surprised} state: $ 6 $
\end{itemize}

\vspace{\baselineskip}
\noindent $ i $ stands for the index of a bin in the feature vector, hence a number from 1 to 420 (420 bins for the feature vector).
\newline

\vspace{\baselineskip}
\noindent $ v $ stands for the value that the bin has at a given index $i$. These values are normalized to fit in interval $ [-1;1] $ so small values will not be overwhelmed by higher values. To normalize the values, the maximum value $max$ of the whole feature vector has to be found as well as the minimum value $min$. To normalize the feature vector , each value has to be processed with the following formula: 
\newline

\begin{equation}
new\_v = -1 + (1 - (-1))\times\frac{v - min}{max - min}
\end{equation}

\noindent where,

\begin{itemize}
  \item $ v $, the value that the bin has at a given index
  \item $ new\_v $, the new normalized value
  \item $ -1 $, the minimum of the normalized interval
  \item $ 1 $, the maximum of the normalized interval
  \item $ min $, the minimum $ v $ value of the whole feature vector
  \item $ max $, the maximum $ v $ value of the whole feature vector
\end{itemize}

\vspace{\baselineskip}
\noindent The feature vector has to be ended by the pair $ -1:-1 $.

\phantomsection
\section{Training and Model}

\vspace{\baselineskip}
\noindent The training part uses images from databases. The database used is the Karolinska Directed Emotional Faces Database (KDEF) database, as described in Chapter \ref{ch:motivations}. All face images are sorted by emotions. To compute feature vectors for train data, 7 folders have to be browsed, one for each emotion.
\newline

\noindent While browsing all folders, a LBP feature vector is computed for each image, and added to a file, which will eventually contain all feature vectors of the training set. The SVM model will then be trained using these feature vectors.
\newline

\noindent All four kernels available were tested to find the best one. As presented in Equation \ref{kernels_svm}, the four kernels are the following:

\begin{itemize}
  \item Linear:
  \begin{equation}
K(x_i,x_j) = x_i^Tx
\end{equation}

  \item Polynomial:
  \begin{equation}
K(x_i,x_j) = (\gamma x_i^Tx_j + r)^T, \gamma > 0
\end{equation}

  \item Radial basis function (RBF)
  \begin{equation}
K(x_i,x_j) = \exp(-\gamma \| x_i - x_j \|^2), \gamma > 0
\end{equation}

  \item Sigmoid:
  \begin{equation}
K(x_i,x_j) = \tanh(\gamma x_i^T x_j + r)
	\end{equation}
\end{itemize}

\vspace{\baselineskip}
\noindent For the three last kernels, the $\gamma$ parameter can be changed and optimized. Each of these kernels was tested with the default $\gamma$ parameter, and then with an optimized one. The optimized $\gamma$ parameter has been computed using an optimization tool.
\newline

\noindent For the polynomial kernel, the $D$ parameter represents the dimension of the polynomial. It has to be chosen. Once it is chosen the $ \gamma $ is found automatically.
\newline

\noindent For the RBF and the sigmoid kernel, two parameters has to be chosen: the cache parameter and the $\gamma$ parameter. These parameters can be found by using \textit{gridsearch}, a script of the SVM library. They are the most optimized for the RBF kernel and for the sigmoid kernel.
\newline

\noindent The cross validation method was also used. It is a method that trains and tests the data at the same time by itself in order to find the best parameters possible. It starts by dividing the dataset into $ n $ parts, and one part is used for testing while the other are used for training. The cross validation method has $n$ training/testing cycles, each cycle using a different testing set.  
\newline






